{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "upset-collection",
   "metadata": {},
   "source": [
    "# Perception III: Detection â€“ Practice\n",
    "\n",
    "## 2D Object Detection with YOLO\n",
    "\n",
    "### Agenda\n",
    "1. YOLOv3\n",
    "2. Dataset and labels\n",
    "3. YOLO - Detection\n",
    "4. YOLO - Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verbal-portfolio",
   "metadata": {},
   "source": [
    "# 1. YOLOv3\n",
    "## Recap lecture: YOLOv1\n",
    "\n",
    "![01_yolov1](./doc/01_yolov1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinct-edward",
   "metadata": {},
   "source": [
    "## The following code is based on YOLOv3 (tiny):\n",
    "\n",
    "![03_yolov3](./doc/03_yolov3.png)\n",
    "\n",
    "### Main differences of YOLOv1 and YOLOv3:\n",
    "* v3 has multiple (here: 2) output tensors at different scales to detect objects with different sizes\n",
    "* v3 uses three anchor boxes in every grid cell: these anchor boxes have predefined sizes/ratios\n",
    "\n",
    "![04_yolov3](./doc/04_yolov3.png)\n",
    "\n",
    "### Anchor boxes:\n",
    "\n",
    "![05_yolov3](./doc/05_yolov3.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "returning-charter",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import argparse\n",
    "import datetime\n",
    "import os\n",
    "import time\n",
    "\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from models import *\n",
    "from utils.datasets import *\n",
    "from utils.transforms import *\n",
    "from utils.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collect-spring",
   "metadata": {},
   "source": [
    "# 2. Dataset and labels\n",
    "\n",
    "## KITTI\n",
    "\n",
    "Training dataset: 7481 labelled images \n",
    "\n",
    "http://www.cvlibs.net/datasets/kitti/\n",
    "![06_kitti.png](./doc/06_kitti.png)\n",
    "\n",
    "![08_kitti.jpg](./doc/08_kitti.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rough-recommendation",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Define kitti data directories\n",
    "data_dir = \"doc/kitti/\"\n",
    "img_dir = os.path.join(data_dir, \"images\")\n",
    "label_dir = os.path.join(data_dir, \"labels\")\n",
    "\n",
    "# Initialize plot\n",
    "fig, axs = plt.subplots(len(os.listdir(img_dir)), figsize=(20, 35))\n",
    "\n",
    "# Plot images and labels\n",
    "for idx, img_name in enumerate(os.listdir(img_dir)):\n",
    "    # Read image from disk\n",
    "    im = Image.open(os.path.join(img_dir, img_name))\n",
    "    axs[idx].imshow(im)\n",
    "\n",
    "    # Read labels from disk and plot 2D bounding box with class name\n",
    "    with open(os.path.join(label_dir, img_name.split(\".\")[0] + \".txt\")) as label_file:\n",
    "        labels = label_file.readlines()\n",
    "        for label in labels:\n",
    "            label_data = label.split(\" \")\n",
    "\n",
    "            # Extract class (cls) and box top left corner (x1, y1) and box dimensions (box_w, box_h)\n",
    "            cls = label_data[0]\n",
    "            x1, y1 = float(label_data[4]), float(label_data[5])\n",
    "            box_w, box_h = (\n",
    "                float(label_data[6]) - float(label_data[4]),\n",
    "                float(label_data[7]) - float(label_data[5]),\n",
    "            )\n",
    "\n",
    "            bbox = patches.Rectangle(\n",
    "                (x1, y1), box_w, box_h, linewidth=2, edgecolor=\"r\", facecolor=\"none\"\n",
    "            )\n",
    "            axs[idx].add_patch(bbox)\n",
    "            axs[idx].text(\n",
    "                x1,\n",
    "                y1,\n",
    "                s=cls,\n",
    "                color=\"white\",\n",
    "                verticalalignment=\"top\",\n",
    "                bbox={\"color\": \"r\", \"pad\": 0},\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "academic-paper",
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in labels:\n",
    "    print(\n",
    "        label\n",
    "    )  # type, truncated, occluded, alpha, 2D bbox (x1, y1, x2, y2), dim (h, w, l), 3D loc (x, y, z), rotation_y, score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fiscal-bailey",
   "metadata": {},
   "source": [
    "Labels for YOLOv3:\n",
    "* Box center (x,y) + Box width/height instead of box edges (x1, y1, x2, y2)\n",
    "* x, y, w, h are relative to image size\n",
    "![07_labels.png](./doc/07_labels.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessible-nigeria",
   "metadata": {},
   "source": [
    "# 3. YOLO - Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "endless-sheriff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Options\n",
    "img_path = \"./data/kitti/images/test/002114.png\"  # feel free to try a different image in this folder!\n",
    "model_def = \"./config/yolov3-kitti-tiny.cfg\"\n",
    "weights_path = \"./weights/yolov3-kitti-tiny.pth\"\n",
    "class_path = \"./data/kitti/classes.names\"\n",
    "conf_thres = 0.8\n",
    "nms_thres = 0.4\n",
    "iou_thres = 0.5\n",
    "n_cpu = 0\n",
    "img_size = 352"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "harmful-chancellor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define device\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "illegal-grave",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Load image\n",
    "img = np.array(Image.open(img_path).convert(\"RGB\"), dtype=np.uint8)\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "whole-strap",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Placeholder\n",
    "boxes = np.zeros((1, 5))\n",
    "\n",
    "img, _ = transforms.Compose([DEFAULT_TRANSFORMS, Resize(img_size)])((img, boxes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deluxe-relevance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up model\n",
    "model = Darknet(model_def, img_size=img_size).to(device)\n",
    "\n",
    "# Load checkpoint weights\n",
    "model.load_state_dict(torch.load(weights_path, map_location=torch.device(\"cpu\")))\n",
    "\n",
    "# Set model in evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alleged-reality",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load class names\n",
    "classes = load_classes(class_path)\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focused-density",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure input\n",
    "img = Variable(img.type(torch.FloatTensor))\n",
    "img.unsqueeze_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aquatic-exhibition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get detections\n",
    "with torch.no_grad():\n",
    "    detections = model(img)\n",
    "    print(detections.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "angry-italian",
   "metadata": {},
   "outputs": [],
   "source": [
    "detections = non_max_suppression(detections, conf_thres, nms_thres)\n",
    "print(detections[0].shape)\n",
    "print(detections[0])  # x1, y1, x2, y2, conf, class1, class2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sized-siemens",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Bounding-box colors\n",
    "cmap = plt.get_cmap(\"tab20b\")\n",
    "colors = [cmap(i) for i in np.linspace(0, 1, 20)]\n",
    "\n",
    "# Plot detections\n",
    "# Create plot\n",
    "img = np.array(Image.open(img_path))\n",
    "fig, ax = plt.subplots(1, figsize=(15, 15))\n",
    "ax.imshow(img)\n",
    "\n",
    "# Draw bounding boxes and labels of detections\n",
    "if detections is not None:\n",
    "    # Rescale boxes to original image\n",
    "    detections_rescaled = rescale_boxes(detections[0], img_size, img.shape[:2])\n",
    "    unique_labels = detections_rescaled[:, -1].cpu().unique()\n",
    "    n_cls_preds = len(unique_labels)\n",
    "    bbox_colors = random.sample(colors, n_cls_preds)\n",
    "    for x1, y1, x2, y2, conf, cls_conf, cls_pred in detections_rescaled:\n",
    "\n",
    "        print(\"Label: %s, Conf: %.5f\" % (classes[int(cls_pred)], cls_conf.item()))\n",
    "\n",
    "        box_w = x2 - x1\n",
    "        box_h = y2 - y1\n",
    "\n",
    "        color = bbox_colors[int(np.where(unique_labels == int(cls_pred))[0])]\n",
    "        # Create a Rectangle patch\n",
    "        bbox = patches.Rectangle(\n",
    "            (x1, y1), box_w, box_h, linewidth=2, edgecolor=color, facecolor=\"none\"\n",
    "        )\n",
    "        # Add the bbox to the plot\n",
    "        ax.add_patch(bbox)\n",
    "        # Add label\n",
    "        plt.text(\n",
    "            x1,\n",
    "            y1,\n",
    "            s=classes[int(cls_pred)],\n",
    "            color=\"white\",\n",
    "            verticalalignment=\"top\",\n",
    "            bbox={\"color\": color, \"pad\": 0},\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documented-miniature",
   "metadata": {},
   "source": [
    "## Detection and plot loop for KITTI sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "union-mediterranean",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "fig = plt.figure(figsize=(12, 12))\n",
    "ax = fig.add_subplot(111)\n",
    "plt.ion()\n",
    "fig.show()\n",
    "fig.canvas.draw()\n",
    "colors = [\"r\", \"b\"]\n",
    "\n",
    "img_dir = \"data/kitti/seq01\"\n",
    "\n",
    "for img_name in os.listdir(img_dir):\n",
    "    img_path = os.path.join(img_dir, img_name)\n",
    "\n",
    "    img = np.array(Image.open(img_path).convert(\"RGB\"), dtype=np.uint8)\n",
    "    img_shape = img.shape[:2]\n",
    "\n",
    "    # Create plot\n",
    "    ax.clear()\n",
    "    ax.imshow(img)\n",
    "\n",
    "    # Label Placeholder\n",
    "    boxes = np.zeros((1, 5))\n",
    "\n",
    "    img, _ = transforms.Compose([DEFAULT_TRANSFORMS, Resize(img_size)])((img, boxes))\n",
    "\n",
    "    # Configure input\n",
    "    img = Variable(img.type(torch.FloatTensor))\n",
    "    img.unsqueeze_(0)\n",
    "\n",
    "    # Get detections\n",
    "    with torch.no_grad():\n",
    "        detections = model(img)\n",
    "        detections = non_max_suppression(detections, conf_thres, nms_thres)\n",
    "\n",
    "    # Draw bounding boxes and labels of detections\n",
    "    if detections[0] is not None:\n",
    "        # Rescale boxes to original image\n",
    "        detections = rescale_boxes(detections[0], img_size, img_shape)\n",
    "        for x1, y1, x2, y2, conf, cls_conf, cls_pred in detections:\n",
    "            box_w = x2 - x1\n",
    "            box_h = y2 - y1\n",
    "            color = colors[int(cls_pred)]\n",
    "\n",
    "            # Create a Rectangle patch\n",
    "            bbox = patches.Rectangle(\n",
    "                (x1, y1), box_w, box_h, linewidth=2, edgecolor=color, facecolor=\"none\"\n",
    "            )\n",
    "            # Add the bbox to the plot\n",
    "            ax.add_patch(bbox)\n",
    "            # Add label\n",
    "            plt.text(\n",
    "                x1,\n",
    "                y1,\n",
    "                s=classes[int(cls_pred)] + str(cls_conf),\n",
    "                color=\"white\",\n",
    "                verticalalignment=\"top\",\n",
    "                bbox={\"color\": color, \"pad\": 0},\n",
    "            )\n",
    "    fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatal-dominant",
   "metadata": {},
   "source": [
    "# 4. YOLO - Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sudden-skill",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "fig = plt.figure(figsize=(12, 12))\n",
    "ax = fig.add_subplot(111)\n",
    "plt.ion()\n",
    "fig.show()\n",
    "fig.canvas.draw()\n",
    "colors = [\"r\", \"b\"]\n",
    "\n",
    "img_dir = \"data/kitti/images/test\"\n",
    "label_dir = \"data/kitti/labels/test\"\n",
    "\n",
    "labels = []\n",
    "metrics = []\n",
    "\n",
    "for img_name in os.listdir(img_dir):\n",
    "    img_path = os.path.join(img_dir, img_name)\n",
    "    label_path = os.path.join(label_dir, img_name[:-4] + \".txt\")\n",
    "\n",
    "    # Load images\n",
    "    img = np.array(Image.open(img_path).convert(\"RGB\"), dtype=np.uint8)\n",
    "    img_shape = img.shape[:2]\n",
    "\n",
    "    # Load boxes\n",
    "    boxes = np.loadtxt(label_path).reshape(-1, 5)\n",
    "\n",
    "    # Create plot\n",
    "    ax.clear()\n",
    "    ax.imshow(img)\n",
    "\n",
    "    img, boxes = transforms.Compose([DEFAULT_TRANSFORMS, Resize(img_size)])(\n",
    "        (img, boxes)\n",
    "    )\n",
    "\n",
    "    # Extract labels\n",
    "    labels += boxes[:, 1].tolist()\n",
    "    # Rescale boxes\n",
    "    boxes[:, 2:] = trans_xywh2xyxy(boxes[:, 2:])\n",
    "    boxes[:, 2:] *= img_size\n",
    "\n",
    "    # Configure input\n",
    "    img = Variable(img.type(torch.FloatTensor))\n",
    "    img.unsqueeze_(0)\n",
    "\n",
    "    # Get detections\n",
    "    with torch.no_grad():\n",
    "        detections = model(img)\n",
    "        detections = non_max_suppression(detections, conf_thres, nms_thres)\n",
    "\n",
    "    metrics += get_batch_statistics(\n",
    "        detections, boxes, iou_threshold=iou_thres\n",
    "    )  # true_positives, pred_scores, pred_labels\n",
    "\n",
    "    # Draw bounding boxes and labels of detections\n",
    "    if detections[0] is not None:\n",
    "        # Rescale boxes to original image\n",
    "        detections = rescale_boxes(detections[0], img_size, img_shape)\n",
    "        for x1, y1, x2, y2, conf, cls_conf, cls_pred in detections:\n",
    "            box_w = x2 - x1\n",
    "            box_h = y2 - y1\n",
    "            color = colors[int(cls_pred)]\n",
    "\n",
    "            # Create a Rectangle patch\n",
    "            bbox = patches.Rectangle(\n",
    "                (x1, y1), box_w, box_h, linewidth=2, edgecolor=color, facecolor=\"none\"\n",
    "            )\n",
    "            # Add the bbox to the plot\n",
    "            ax.add_patch(bbox)\n",
    "            # Add label\n",
    "            plt.text(\n",
    "                x1,\n",
    "                y1,\n",
    "                s=classes[int(cls_pred)],\n",
    "                color=\"white\",\n",
    "                verticalalignment=\"top\",\n",
    "                bbox={\"color\": color, \"pad\": 0},\n",
    "            )\n",
    "    fig.canvas.draw()\n",
    "\n",
    "# Concatenate sample statistics\n",
    "true_positives, pred_scores, pred_labels = [\n",
    "    np.concatenate(x, 0) for x in list(zip(*metrics))\n",
    "]\n",
    "precision, recall, AP, f1, ap_class = ap_per_class(\n",
    "    true_positives, pred_scores, pred_labels, labels\n",
    ")\n",
    "\n",
    "print(\"Average Precisions:\")\n",
    "for i, c in enumerate(ap_class):\n",
    "    print(f\"+ Class '{c}' ({classes[c]}) - AP: {AP[i]}\")\n",
    "\n",
    "print(f\"Mean Average Precision mAP: {AP.mean()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
