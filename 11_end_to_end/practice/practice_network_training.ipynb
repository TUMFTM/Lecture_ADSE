{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# End-to-End Learning | Practical Session Part II | Training Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import gym.wrappers\n",
    "\n",
    "from nets.imitator import Policy\n",
    "from dataset import MarkovProcess\n",
    "from early_stopping import EarlyStopping\n",
    "from utils import make_Dirs, save_Result, get_EnvInfo\n",
    "from nets.abstract import Abstract\n",
    "\n",
    "ratio = [16, 10]\n",
    "plt.rcParams[\"figure.figsize\"] = ratio\n",
    "plt.rcParams.update({\"font.size\": 22})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Imitator Policy\n",
    "We use a simple network for the imitator policy. Two consecudive convolutional layers are followed by MaxPooling and flattening.\n",
    "Afterwards two fully connected layers are inserted. The last layer has as activation function the Tanh function.\n",
    "\n",
    "![Tanh Function](resources/tanh.png)\n",
    "\n",
    "As loss function we use the L2 norm between expert data and predicted one:\n",
    "\n",
    "$ l(x,y) = L = {l_1, ..., L_N}^T, l_n = (x_n - y_n)^2 $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Policy(Abstract):\n",
    "    def __init__(\n",
    "        self,\n",
    "        s_dim,\n",
    "        a_dim,\n",
    "        opt_params={\n",
    "            \"lr\": 1e-3,\n",
    "            \"eps\": 1e-8,\n",
    "            \"weight_decay\": 0.0,\n",
    "            \"amsgrad\": False,\n",
    "            \"tadam\": False,\n",
    "        },\n",
    "        use_cuda=False,\n",
    "    ):\n",
    "        super(Policy, self).__init__(s_dim, a_dim, use_cuda)\n",
    "\n",
    "        self.loss = nn.MSELoss(reduction=\"sum\")\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(32 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 3)\n",
    "\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        nns = self._modules.items()\n",
    "        self.init(\"policy\", nns, opt_params)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        input = self.convert(x, self.s_dim)\n",
    "\n",
    "        h1 = self.pool(F.relu(self.conv1(input)))\n",
    "        h2 = self.pool(F.relu(self.conv2(h1)))\n",
    "        h3 = h2.view(-1, 32 * 5 * 5)\n",
    "        h4 = F.relu(self.fc1(h3))\n",
    "        h5 = F.relu(self.fc2(h4))\n",
    "        action = self.tanh(self.fc3(h5))\n",
    "\n",
    "        return action\n",
    "\n",
    "    def criterion(self, a_imitator, a_exp):\n",
    "        loss = self.loss(a_imitator, a_exp)\n",
    "        return loss\n",
    "\n",
    "    def reset(self):\n",
    "        return np.zeros(self.a_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def epoch_policy(dataset, policy, n_epoch, mode):\n",
    "    loss_sum = 0.0\n",
    "    loss_num = 0\n",
    "    if \"train\" in mode:\n",
    "        policy.train()\n",
    "    else:\n",
    "        policy.eval()\n",
    "    for batch_idx, (so, a_) in enumerate(dataset):\n",
    "        output = policy(so)\n",
    "        loss = policy.criterion(output, a_).mean()\n",
    "        if policy.training:\n",
    "            policy.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            policy.optimizer.step()\n",
    "        loss_sum += loss.data.item()\n",
    "        loss_num += 1\n",
    "    loss_sum /= loss_num\n",
    "    print(\n",
    "        \"{}-th epoch {} of policy was end: \\n\\tloss = {}\".format(\n",
    "            n_epoch, mode, loss_sum\n",
    "        )\n",
    "    )\n",
    "    return loss_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "DATA_NAME = \"./expert/summary.csv\"\n",
    "METHOD = \"Lecture_End_to_End_PracticeSession\"\n",
    "SAVE_DIR = \"./result/\" + METHOD + \"/\"\n",
    "make_Dirs(SAVE_DIR)\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "TRAIN_VALID = 0.95\n",
    "EARLY_LENGTH = 5\n",
    "N_EPOCH = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# prepare environment and agent (imitator)\n",
    "env = gym.make(\"CarRacing-v0\")\n",
    "s_dim, a_dim, transform = get_EnvInfo(env)\n",
    "\n",
    "# specify the random seeds\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "env.seed(0)\n",
    "# set expert dataset\n",
    "dataset = MarkovProcess(DATA_NAME, transform=transform)\n",
    "len_train = int(TRAIN_VALID * len(dataset))\n",
    "train_loader, valid_loader = torch.utils.data.random_split(\n",
    "    dataset, [len_train, len(dataset) - len_train]\n",
    ")\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_loader, batch_size=BATCH_SIZE, shuffle=True\n",
    ")\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    valid_loader, batch_size=BATCH_SIZE, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Show Example Training Data\n",
    "State is resized from 96x96x3 RGB image to a grayscale 32x32x1 image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for batch_idx, (so, a_) in enumerate(dataset):\n",
    "    print(\"The internal representation of the state: \")\n",
    "    print(\"\\n\")\n",
    "    print(so)\n",
    "    print(\"\\n\")\n",
    "    print(\"The executed action at this timestamp was: \", a_)\n",
    "    plt.imshow(so.permute(1, 2, 0), cmap=\"gray\")\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Training of the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "policy = Policy(s_dim, a_dim)\n",
    "stopper = EarlyStopping(length=EARLY_LENGTH)\n",
    "\n",
    "# prepare buffers to store results\n",
    "train_loss_policy = []\n",
    "valid_loss_policy = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# optimize policy by the expert dataset\n",
    "print(\"Start learning policy!\")\n",
    "for n_epoch in range(1, N_EPOCH + 1):\n",
    "    train_loss_policy.append(epoch_policy(train_loader, policy, n_epoch, \"train\"))\n",
    "    valid_loss_policy.append(epoch_policy(valid_loader, policy, n_epoch, \"valid\"))\n",
    "    # early stopping\n",
    "    if stopper(valid_loss_policy[-1]):\n",
    "        print(\"Early Stopping to avoid overfitting!\")\n",
    "        break\n",
    "print(\"Finished learning policy!\")\n",
    "# save trained model\n",
    "policy.release(SAVE_DIR)\n",
    "# close everything\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Plotting the Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "training_data = {\"train\": train_loss_policy, \"valid\": valid_loss_policy}\n",
    "save_Result(SAVE_DIR, \"loss_policy\", training_data)\n",
    "plt.clf()\n",
    "for key, val in training_data.items():\n",
    "    plt.plot(range(len(val)), val, label=key)\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
